---
title: 人工智能的可解释性
date: 2019-10-20
categories:
   - 倏尔·论
tags:
   - DL
---

::: tip

虽然深度学习技术在近几年发展的非常之快，但是没有谁能说明白 AI 到底做了什么，怎么做的，这就是 AI 缺乏可解释性的表现，而且很多时候 AI 也只是被我们当做一个黑箱子使用了，那么可解释性真的那么重要吗？

:::

<!-- more -->

## 可解释性有什么用？

想一个简单的问题，如果有一个 AI 医生给你做手术，你敢把自己的生命交给他吗？

至少以现在的技术来说，你一定会也应该拒绝的，现在的深度学习技术非常缺乏可解释性，你根本不知道它会不会有什么”意外“的举动，这样的 AI 怎么值得托付生命呢？

另外，可解释性对于神经网络的开发也有着很大的帮助，当前我们在开发的时候大多情况仅仅考虑其输入输出，而中间的网络结构大多仅仅是根据运行效果手动调优，但是如何调优？很大程度是凭借经验，因为你很难知道你的一点点微小改动到底会给整个网络带来什么样的改变，更不知道为什么会带来这样的改变，这也是为什么神经网络也被戏称为”炼丹“，深度学习被戏称为深度玄学了

## 那么如何研究可解释性？

一种简单的方法是使用反卷积对神经网络学到的特征进行可视化，这个这里不详谈，这里简述下 $Stanford University$ 和 $Google Brain$ 的研究人员的一项最新研究成果，自动提取视觉概念的算法 ACE

![Explainable_AI_01](../Images/Explainable_AI_01.png)

上面就是整个算法的流程图了，不过这一步一步是怎么做的呢？

考虑一个问题，AI 是怎么识别图片的呢？我们可以猜想它可能会抓住一张图片中的一些**重要概念**，比如在它观察到两个眼睛一个鼻子一张嘴后就会认为这是一个人，当然，同一类图应当有着类似的概念，这个类似怎么评估呢？我们可以通过聚类，通过欧式距离进行评估

就像刚刚所说的那样，为了兼顾细粒度和粗粒度的特征，我们首先对图片在不同大小下进行分割，将各个图片块输入某个训练好的网络之后，使用聚类对某一层输出（一般取一个中间层）提取出各个不同的概念，然后可以根据分布得到哪些是重要概念

![Explainable_AI_02](../Images/Explainable_AI_02.png)

可以看到，网络很大程度上会根据轮胎、logo 等因素判断出这是一辆警车

我们不妨猜想网络对图片的识别主要基于刚刚识别到的重要概念，因此研究者对重要概念进行了移除和添加，研究结果表明，如果重要概念存在，则分类器能够很容易分类正确，而移除重要概念的图片很多都被误分类了，可见重要概念在很大程度上决定了神经网络的判断

## 可解释性与什么有关？

可解释性的影响因素问题可参考 `Network Dissection: Quantifying Interpretability of Deep Visual Representations`

### 网络结构和监督对可解释性的影响

![Explainable_AI_03](../Images/Explainable_AI_03.png)

通过观察我们可以发现，较深的网络可解释性会增强

另外，相比于监督学习，自监督模型会产生更多的纹理检测器，但目标检测器较少

### 训练条件对可解释性的影响

![Explainable_AI_04](../Images/Explainable_AI_04.png)

可以发现，无 dropout 会有着很多的纹理检测器，而 BN 的各个层次检测器都很少，可解释性很差，虽然其加速了训练，但是它会对每层的响应进行白化，平滑了他们的 scale，网络能够在训练过程中更容易地旋转中间表征，直接导致了旋转对可解释性会造成影响

# References

1. [AI 眼中的世界是什么样子？谷歌新研究找到了机器的视觉概念 - 机器之心](https://mp.weixin.qq.com/s/JXJYvnLqLLuSclsd4ZTySA)
2. [神经网络的可解释性](https://blog.csdn.net/isMarvellous/article/details/75900055)
