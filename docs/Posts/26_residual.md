---
title: 残差结构的作用
date: 2020-01-18
categories:
   - 思
tags:
   - DL
---

::: tip

这并不是讲解残差网络的一篇文章，而是对残差结构对神经网络的一些作用的感悟，很多想法是来自于其它网络诸如 DenseNet、Xception 等等的启发

:::

## 何谓残差

残差网络通过“Shortcut”连接使得网络不会因为层次太深导致的梯度难以传播的问题

当模型不需要太深的结构时，完全可以只使用“Shortcut”，这使得被连接的卷积模块被“短路”掉（事实上是该模块自己 train 出来一个较小的值），这样就使得网络有着在深度上的一种选择性

## 残差的作用

当然，残差固然有着上述作用，但是除此之外呢？如果说该模块并没有被“短路”呢？我们可以认为它不仅学习到了新的特征，还保留了旧的特征，这使得下一层可以同时使用低层次特征与高层次特征进行组合，学习出来更加丰富的特征，当然，如果是这样的话，高低层次特征的组合方式使用通道上 Concatenate 更加合理一点，而不是 Addition（ResNext 也提到了通道上 Concatenate 方法，Inception 的基础也是这样）

此外，ResNet 能够组合的层次仅仅限于第 n 层与第 n+2 层之间，如果我们每一层就建立一个残差而不是像 ResNet 中每两层建立一个残差，另外，我们可以认为不加激活的线性变换就可以称之为残差，那么每一层就成了 $g(w_1x) + w_2x$，那么低层次完全有可能流到更高的层次去，使得网络能够更加轻易的学到更加复杂的特征

> 基于上面的想法以及卷积空间上面的一些心得，我在“恒锐杯”竞赛中建立了[一种网络结构](https://zsync.github.io/shoeprint-recognition/#/model?id=???)

## DenseNet 结构

再之后，我了解到了 DenseNet 这种结构，说实话，没什么新奇的东西，只不过是一种对残差的利用比较极致的结构，这和我前面设计的那种网络结构有着一定的相似性，特别是其中提到的“特征重用”的效果

但是这种 Dense 的设计方法让我对网络结构的演化产生了兴趣

我们传统的网络结构，无论是传统网络、CNN、RNN，说到底其在深度方向都是多个二分图堆积而成而已，比如一个 $3 \to 4 \to 5$ 的单隐层网络结构，其图结构不过是 $3 \to 4$ 与 $4 \to 5$ 的两张二分图的结合，那么何谓残差呢？“残差”（每层都建立一个残差）就是额外建立一个 $3 \to 5$ 的二分图，也许你会说 $3 \to 4$ 和 $4 \to 5$ 可以学到 $3 \to 5$ 的特征，但是这个 $3 \to 5$ 是比两层学到的少一层激活的，也就是较低层次的特征，这样高低层次特征组合

这样，我有了一个大胆的想法，将这些结点表示在一张图里如何？

$$
\begin{bmatrix}
0 & 0 & 0 & a & a & a & a & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & a & a & a & a & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & a & a & a & a & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

先来一个不带残差的，很明显，这必然是一个上三角阵，而且各个线性变换模块分布在对角线上面

$$
\begin{bmatrix}
0 & 0 & 0 & a & a & a & a & c & c & c & c & c \\
0 & 0 & 0 & a & a & a & a & c & c & c & c & c \\
0 & 0 & 0 & a & a & a & a & c & c & c & c & c \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & b & b & b & b & b \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

带残差的瞬间将整个上三角阵给填满了（线性变换模块下仍有锯齿状空隙），如果多表示几层，可以很容易的发现，DenseNet 便是这种满的上三角阵结构

不难想到，如果我们将下面的锯齿部分也填满的话，那么使用这一个图便可以表示极其复杂的网络结构，它的网络结构并不再是固定的，我们可以让它自行学习

至于如何表示输入输出，如何表示一个深度网络，如何激活，这些问题都还待进一步研究，暂时都没有比较好的方案，而且这样必然会带来大量的冗余运算，也许没有任何可实施性

但是，在这种层级上可以看到 ResNet 到 DenseNet 的必然性，但基于 DenseNet 是否可以进一步发展呢？这个问题且留给未来
