---
title: 注意力机制与 Mask 机制
date: 2020-01-18
categories:
   - 思
tags:
   - DL
---

::: tip

NLP 中大火的 Attention 机制在其他领域（CNN 与 GNN 等）中也有着不小的应用趋势，但是 Attention 本质上是个什么机制呢？

:::

## Mask 机制

在 Attention 机制之前，我们首先看一下 Mask 机制，这是一种很常见的机制，相当于对原有特征乘上一定的权重，如果这个权重为二值（0 or 1）的话，也可以称为 Gate 机制，这可以非常方便地对原有特征进行过滤

比如在 MobileNet 中，就将 GlobalAveragePooling 改为 DepthwiseConv，这为每个通道在空间的各个位置上分配了一定的权重，这防止了 GlobalAveragePooling 导致的对特征所处位置不敏感的问题

MobileNet 本身很容易学到一个较好的 Mask，这个 Mask 可能会将更多的注意力放在中间部分而不是周围的背景部分，这使得网络在空间上有着一个简单的注意力分配过程，虽然这还称不上注意力机制

但这种方法有着一个很明显的缺陷，对于同一个特征，永远只有相同的 Mask，如果一张图片的关键目标分布在模版权重低的位置，那么将会有很大概率识别错，也就是对图片进行简单地平移将会产生不同的结果

## SE-Net

SE-Net 中使用了通道的注意力机制，它在每一层首先将每个通道都压缩为一个 Scale，这个压缩操作完全可以使用简单的 GlobalAveragePooling 或者其他复杂的操作，然后对这 num_channels 个 Scale 进行 Dense 全连接（到 num_channels 个单元），最后对它们进行 Sigmoid 激活，这样结果就是 0~1 之间的 Mask 了，然后将这个 Mask 乘上原本的值

难道传统的 CNN 结构学不到这样的效果吗，为什么还要手动生成这么一个 Mask？传统的 CNN 还真学不出来这个结果，这与 GlobalAveragePooling 的缺陷一样，每次卷积只会考虑一个局部，它并不能“看到”卷积核之外的东西，通常情况下，我们为了降低参数量，会使用较小的卷积核，但大的卷积核通常是优于小的卷积核的，而 SE-Net 的这种注意力恰恰在一定程度上弥补了小卷积核的缺点，每个通道都能在一定程度上关注到空间的全局

当然，如果使用 GlobalAveragePooling 进行压缩的话，仍然是对特征的空间分布是不敏感的，我们可以尝试使用其他压缩方法，以弥补该问题

## STN

并非所有 Attention 机制都是一种 Mask 机制，比如 STN，其本质是对图片进行平移、旋转等变换，这样便可以从原图中“注意”到我们感兴趣的目标
