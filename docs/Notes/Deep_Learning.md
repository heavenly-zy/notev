---
sidebarDepth: 2
---
# Deep Learning
<!-- [TOC] -->

## 1 Neural Networks and DeepLearning
### 1.1 Introduction to DeepLearning
#### 1.1.1 Welcome

#### 1.1.2 What is a Neural Network

#### 1.1.3 Supervised Learning with Neural Networks
Structured Data ： Database  
Unstructured Data : Image Audio Text

#### 1.1.4 Why is Deep Learning taking off?
![DeepLearning01](../Images/DeepLearning01.png)  
> 数据量的增加，使得深度学习的优势越来越明显  

![DeepLearning02](../Images/DeepLearning02.png)  
> 迭代过程需要的时间越长，所能实现的想法就越少，因此训练的算法改良对深度学习来说是一个很关键的问题。  
> 比如使用sigmod函数作为激活函数时，过大的输入会使神经网络饱和，梯度下降算法越来越慢；改良为ReLU函数（修正线性单元）后，梯度下降的算法运行的更快，迭代时间相应就减少了很多

#### 1.1.5 About this Course

#### 1.1.6 Course Resources

### 1.2 Basics of Neural Network programming
#### 1.2.1 Binary Classification
* 如果想要识别图片中是否有猫，可以将输入和输出组织成这样：输入为64 * 64 * 3维（像素点64 * 64，三个颜色通道），输出为一个数字（0或者1）
* 我们对输入和输出符号做以下约定：
    * nx为输入的维度，比如这里就是64 * 64 * 3
    * 用X将x按列组织起来
    * 用Y将y按行组织起来
    * 我们可以用Python里的`X.shape()`查看规模

#### 1.2.2 Logistic Regression
1. 相对于输入一个`$X$`，输出`$y$`代表这个图片是不是猫，我们更喜欢它能够输出这个图片是猫的概率，这里便用`$\hat{y}$`表示
2. 我们可以使用`$\hat{y} = w^T+b$`来表示这个关系，其中`$w$`是特征权重，维度与特征向量相同，但是这个函数是关于`$x$`的一个线性函数，这无疑是很糟糕的，因为我们想要的`$\hat{y}$`是一个在0到1之间的数呀
3. 这个时候我们就会使用到**sigmoid**函数了，这个神奇的函数将整个实数域映射到了0-1之间，将线性映射为非线性，那么，继续吧

#### 1.2.3 Logistic Regression Cost Function
![DeepLearning03](../Images/DeepLearning03.png)  
1. 计算误差时，如果使用误差的平方作为损失函数很有可能得到多个“低谷”（很多局部最优），使用梯度下降法将得不到真正最优的解
2. 我们使用`$-(ylog\hat{y}+(1-y)log(1-\hat{y}))$`作为损失函数，至于为什么可以使y分别等于0和1推算一下（y也只有这么两个可取的值）
3. 相对于只作用于某一样本的loss function（损失函数），cost function（成本函数）J是作用于整个样本集上的函数；对于某一样本，我们使用loss function来衡量y与`$\hat{y}$`的差距，相对应的，对于某一样本集，我们使用cost function来衡量W和B的效果

#### 1.2.4 Gradient Descent
1. 因为我们选用了logistic的损失函数，所以我们将得到一个最优解而不是像平方误差函数那样的多个局部最优解，因此，我们随意地初始化这么一个w和b，它最终都会随着梯度下降逐步收敛于我们想要的那个解
2. 所谓梯度下降，就是在每一个状态下沿着下降最快的方向降低
3. 现在先不考虑b，只考虑w，我们很容易根据J对w的导数得到使得J下降的w的方向，所以我们根据这个“方向”对w进行调整，就像：`$w -= \alpha dJ(w)/dw$`，这里的`$\alpha$`称作学习率（learning rate），就是每次更新的步长
4. 扩展到w和b的情况，我们每次迭代就需要执行`$w -=  \alpha \delta J(w,b)/\delta w$`和`$b -=  \alpha \delta J(w,b)/\delta b$`

#### 1.2.5 Derivatives

#### 1.2.6 More Derivative Examples

#### 1.2.7 Computation Graph
前向传播

#### 1.2.8 Derivatives with a Computation Graph
1. 反向传播
2. 链式法则
3. 导数用dvar

#### 1.2.9 Logistic Regression Gradient Descent
1. 首先把正向传播的公式列出来：
    1. `$z=w^T+b$`
    2. `$\hat{y}=a=\sigma (z)$`
    3. `$L(a,y)=-(ylog(a)+(1-y)log(1-a))$`
2. 首先计算`$da$`（a对J的导数，dJ省略掉），我们很容易求得`$\frac{-y}{a}+\frac{1-y}{1-a}$`
3. 之后计算`$dz$`，根据链式法则，我们先计算出`$\frac{da}{dz}=a(1-a)$`，再乘上刚刚算出来的`$da$`，我们便可以得到`$dz=a-y$`，很神奇吧，最终居然这么简单
4. 最后，我们的`$dw_1=x_1dz ;dw_2=x_2dz;db=dz$`，你没看错，就是这么简单

#### 1.2.10 Gradient Descent on m Examples
1. 现在我们把上面的一个样本的算法应用到整个数据集上，很明显的，我们需要一个for循环去遍历m个样本，对每个样本进行累加并求取平均值 
2. 像前面一样的，我们在每一层循环内先正向传播计算J，之后反向传播求导计算各项的导数，这并没有什么不同。
3. 值得注意的的是，虽然我们是在计算每一个样本的，但是w和b是对于整个样本集而言的，所以我们在这里将样本集里所有w加和、b也一样.当然这里只考虑了一个特征的w，如果有多个特征（n个），那么我们就对每个w都这么做，最终会得到`$dw_1$`\~`$dw_n$`还有`$db$`，诶？这不就是我们要求的吗？那么还剩最后一步了
4. 所谓最后一步，不过是让w减去`$\alpha dw$`、d减去`$\alpha db$`，完成梯度下降的任务而已
5. 我们这里有两个for循环（m和n），我们知道，循环是很慢的，如果可以，我们会尽最大能力降低时间复杂度，以获得更高的效率，我们在前面提到过这对想法的实现来说是很重要的一件事，但是具体要怎么提高速度呢，我们继续看下一节——向量化

#### 1.2.11 Vectorization
1. 前面已经提到，如果我们直接使用for循环，那么速度会非常的慢，慢到什么程度呢？一个直观的测试告诉我们，它会比向量化慢上300倍！下面，就让我们使用向量化来取代这奇慢的循环吧
2. 让我们来尝试：
```Python
import numpy as np
a = np.array([1, 2, 3, 4]) # 我们先试试怎么初始化一个数组
```

```Python
a = np.random.rand(1000000)
b = np.random.rand(1000000)
c = np.dot(a, b) # 这样就完成了点乘了？？？
```

#### 1.2.12 More Examples of Vectorization
1. 让我们再看几个例子：
    1. `np.exp(v) # 对每个元素求exp`
    2. `np.log(v) # 对每个元素求log`
    3. `np.abs(v) # 对每个元素求abs`
    4. `np.maximum(v) # 求整个矩阵的最大值`
    5. `v**2 # 对每个元素求平方`
    6. `1/v # 对每个元素求倒数`

2. 现在，我们试着去优化那个有着两个for循环的神经网络过程，我们先简单的优化掉n这个for循环：
```Python
dw = np.zeros((n-x, 1)) # 因为要用矩阵，就将原来对每个w初始化为0改成初始化这样的一个全零矩阵
# ...
for i in range(m):
    # ...
    dw += xi dzi
    db += dzi
dw /= m
```
很明显，我们已经把里面的n循环优化掉了，也就是，可以一次将所有特征看做整体w，而不必去对每个特征分别计算
3. 很轻松地，我们完成了对一个for循环的优化，接下来，让我们把所有的for循环都优化掉吧φ(>ω<*) 

#### 1.2.13 Vectorizing Logistic Regression
1. 在前面我们利用X将各组训练数据按列组织起来，我们现在再看如何利用矩阵对这些数据进行计算
2. 我们先看一个训练集的时候，我们是使用`$z^{(i)} = w^Tx^{(i)}+b$`和`$a^{(i)}=\sigma(z^{(i)})$`对每个训练集的前向传播进行计算的
    1. 这里`$w$`是(n, 1)的矩阵，`$w^T$`自然就是(1, n)的矩阵（也就是上节课说的将n个输入特征组织起来）
    2. 再看`$x^{(i)}$`，这也是一组训练集的数据，对应于n个输入特征，很明显，它是(n, 1)的矩阵
    3. 权重和输入特征矩阵相乘，最后输出的是一个数字（或者说(1, 1)矩阵），我们用它加上b，自然就是我们想要的值
    4. sigmod函数不必说也知道
3. 我们接下来看如何对m个训练集同时处理
    1. `$w$`和`$w^T$`自然是不变的，他们不可能随着训练集的数量增加而增加，所以它仍然是(1, n)的矩阵
    2. 继续看X，不必说也知道它是(n, m)的矩阵了，每一列都是刚刚的一个训练集
    3. 然后我们利用矩阵的乘法使两个矩阵相乘，如果我们还对矩阵乘法有点印象的话，就可以知道结果的i行j列元素对应的就是第一个矩阵的i行与第二个矩阵的j列相乘并求和，看到这里想必大家都明白了，前面的每一行（虽然现在只有这么一行）不正对应于一个训练集的所对应的权重`$w$`嘛，后面的每一列不正对应于一个训练集的输入数据嘛，最后的结果不过是将原来的数据列在了一起，形成最后这样的(1, n)的矩阵，比如说(0, 3)是由特征权重w和第四组训练集相乘得来，也就是原来for循环的第四组
    4. 然后让我们加上b吧，既然每个数据都要加b，那就做一个(1, n)的全b矩阵咯
    5. 然后sigmod又不用说了，还是那么简单
4. 下面我们用Python试一下
```Python
Z = np.dot(w.t, X) + b # 咦？居然直接这样就可以了？原来numpy会自动将它扩展为(1, n)的全b矩阵
A = σ(Z) # 对每个z求sigmoid
```

#### 1.2.14 Vectorizing Logistic Regression's Gradient
1. 上节课仅仅展示了对前向传播的简化示例，下面我们对反向传播进行简化
2. 经过上节课的讲解，我们应该是知道了我们怎么用矩阵将一组训练集组织起来以及为何可以这么组织起来，这样，我们应该很容易将单个训练集的`$dz=a-y$`推到整个训练集的`$dZ=A-Y$`，他们都是(1, m)的矩阵
3. 下面要推的是dw和db，db我们可以很容易的写出来`$db=\frac{1}{m}np.sum(dZ)$`，dw相应地，我们想得到那么一个(1, n)的矩阵代表待优化的m个特征，我们可以由每个训练集的`$dw=xdz$`推出在整个训练集上的`$dw=\frac{1}{m}XdZ^T$`，哦，你会发现这会得到一个(n, 1)的矩阵，但是这没关系，反正我们要的是他们的和，也就是`dw = np.sum(np.dot(X, dZ.T)) / m`，嗯，我们不仅将算式推出来了，还将Python代码写出来了，完美，let's continue.
4. 下面我们就是真正让w和b优化了，嗯，减一下就好，别忘了学习率
5. 我们完美地、不显式使用一次循环地完成了一次迭代，哦，是整个训练集的一次迭代哦，如果我们多次迭代呢？我们暂时还没有简单的方法使他避免使用for循环，那就先使用for循环吧，就那样、简单地、在外面嵌套这么一层，就好了

#### 1.2.15 Broadcasting in Python
1. Python的numpy会对某些不满足加减乘除条件的矩阵进行补全，比如某一维度为1的矩阵，这样使得很多操作会变得更加方便，这种特性就叫做广播

#### 1.2.16 A note on python or numpy vectors
1. 广播特性更多的是给我们带来了很大的灵活性，使我们写代码更加方便，但如果对广播特性并不了解的话，很容易出现我们不可预料的bug
2. 我们看`np.random.randn(5)`，你可能会以为他是一个一维的向量，但是通过print它和它的转置可以发现，他不过是一个数组而已我们也可以通过观察它的括号层数来判断它的维度
3. 使用上面说的数组很容易出现不可预料的错误，可能你会在使用的时候以为他是一个向量，所以就……那么如何避免呢？我们尽量总是使用`np.random.randn(5, 1)`或者`np.random.randn(1, 5)`这样的代码，另外使用几个assert(a.sharp == (5, 1))以保证没有写错，还有我们要敢于使用a.resharp(1, 5)以保证生成的是你想要形状的矩阵

#### 1.2.17 Quick tour of Jupyter/iPython Notebooks
emmmmm，我选择暂时不安装，我还是更喜欢原生的pyshell（idle）

#### 1.2.18 Explanation of logistic regression cost function
1. 我们使用`$\hat{y}$`来输入x时表示y=1的概率，那么，我们可以得出这样的一组式子：
    * `$p(y|x) = \hat{y} $` if y = 1
    * `$p(y|x) = 1-\hat{y} $` if y = 0

哦，p(y|x)是什么鬼，让我们回忆下概率的知识，很容易猜出来这是在x的前提下y的概率，也就是刚刚说的输入x时y的概率，那就很明了了
2. 我们将这两个式子写在一起`$p(y|x) = \hat{y}^y(1-\hat{y})^{(1-y)}$`，我们不需要知道为什么这样写，我们只需要知道这样是对的，仔细看看，对吧？
3. 因为我们的损失函数更多的是关注整体的求导之后的方向性（梯度下降最大的方向），我们使用log这个严格递增的函数加在刚推出来的概率上，得到`$log(p(y|x))=ylog\hat{y}+(1-y)log(1-\hat{y})$`，这个就是我们前面提到的损失函数的相反数`$-L(\hat{y},y)$`。嗯？为什么带个负号？哦我们分别来看，前面我们的**损失函数是什么呢，是计算预测值与实际输出的一个差**，我们想要这个值尽量的小，我们再看看**这里的这个，去掉log可是预测满足实际输出的概率**啊，我们当然希望这个值尽量地大，那么这其中的逻辑与关系便可以解释地清楚了
4. 刚刚我们推的是单样本的损失函数，这次我们求整个样本集的成本函数，**根据最大似然估计法，我们已经出现的这个样本集所对应的概率应当是在该情况下出现概率最大的情况**，我们将所有样本的概率乘在一起，然后一样地取log，这样的话，你会发现每一项都是刚刚所说的损失函数的相反数，我们把负号拿出去，这里我们要的是它取最大值，就是要去掉负号的那个的最小值，这不正是我们想要的成本函数嘛……然后我们再进行适当的缩放，这并不影响它的结果的我们的成本函数便得到了：`$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$`

### 1.3 Shallow neural networks
#### 1.3.1 Neural Network Overview
1. 我们已经学习了逻辑回归，现在我们要学习一下神经网络。至于神经网络，简单地说就是让我们上周学习的神经元排列成一个网络……天啊，一个神经元就要写那么多的代码，还要结个网？？？算了不学了不学了！等等，当然这张网是有规律组织起来的我们先看一下这张网长什么样子：  
![DeepLearning04](../Images/DeepLearning04.png)  
2. emmmmm，原来我们还只有一个圈圈，现在一下子变这么多了，不学了，别拦我(╯>д<)╯⁽˙³˙⁾
3. （偷眼瞧）貌似理解错了，我们原来做的貌似不止这么一个圈圈，我们原来的貌似是去掉了中间这四个圈圈的东西，现在的话，貌似只加上了这么一”层“而已，诶呀，这么说我就放心啦
4. 多了一层会有什么后果呢？事实上我们在每个圈圈的位置都使用w和b对左侧输入进行变换，然后加上激活函数，最左面到中间的一层圈圈这样，中间一层圈圈到最右面一层（个）圈圈也是这样
5. 那么中间这4个圈圈怎么计算呢，我们只做过n个输入到1个输出的情况呀。我们先只看一个圈圈，它经过w和b会得到一个值，后面几个圈圈也是这样呀，不过对他们处理的w和b是不一样的，我们使用下标进行区分，我们已经学过了怎么使用矩阵组织多维数据，那么这里我们可以很方便地将各个w组织起来
6. 先讲到这里啦，不然下节课没得讲了(～￣▽￣)～ 

#### 1.3.2 Neural Network Representation
1. 现在讲讲层与层之间的关系，我们学过了相邻两层之间的关系，也就是说知道了0-1和1-2的方法，那么还需要知道更多吗？
2. 哦，我只想说没了，有的话，就是记得上标（带中括号的小数字）代表层数，输入层一般不算作一层，或者叫第0层`$^{[0]}$`
3. 现在我们梳理一下各层的维度吧，以便更加直观地理解各层之间的关系：

`$x$`(`$a^{[0]}$`)|`$W^{[1]}$`|`$b^{[1]}$`|`$a^{[1]}$`|`$W^{[2]}$`|`$b^{[2]}$`|`$\hat{y}$`(`$a^{[2]}$`)
---|---|---|---|---|---|---
`$(n^{[0]}, 1)$`|`$(n^{[1]}, n^{[0]})$`|`$(n^{[1]}, 1)$`|`$(n^{[1]}, 1)$`|`$(1, n^{[1]})$`|`$(1, 1)$`|`$(1, 1)$`
(3, 1)|(4, 3)|(4, 1)|(4, 1)|(1, 4)|(1, 1)|(1, 1)
大概的看一下，我们可以发现，现在满足`$a^{[i]}=\sigma(Wa^{[i-1]}+b^{[i]})$`，而原来每个式子应该是满足`$a^{[i]}=\sigma(w^Ta^{[i-1]}+b^{[i]})$`，貌似差了一个转置，为啥呢？下节就可以知道了

#### 1.3.3 Computing a Neural Network's output
1. 嗯，我们先将我们以前的式子列出来`$z^{[i]}=w^Ta^{[i-1]}+b^{[i]}$`，不要管i，现在只看某两层之间的关系，那么我们可以通过加下标的方式用以区分各个结点，我们很容易写出来这样的`$n^{[i]}$`个式子，我们将它按行组织起来，就形成了`$W^{[i]}$`了呀，哦上节里面的转置便是差在了这里，W本来就是将w的转置组织起来的，自然不需要再转置。
2. 很明显，我们已经在上节把需要推的推出来了，不过当前仅仅针对一次训练数据，我们再看看如何组织数据使得一次可以支持整个训练集的训练

#### 1.3.4 Vectorizing across multiple examples
1. 和之前一样的，我们用列组织训练集的数据，每一列是一次训练数据，我们在上标上加一个带括号的数字来表示这是第几次训练，比如`$^{(3)}$`代表第三次训练
2. 需要我们组织的有哪些数据呢，首先就是输入的x，它会被组织成X，相应地，输出项y会被组织成Y，哦，我们还有考虑中间项，比如z、a，事实上我们只有z和a，x和y均可以用a来表示
3. 我们再看看刚刚我们组织的A和Z，他们各个位置上分别代表什么呢？
    * 从左到右是按列组织的训练集各项，他们用右上角`$^{(i)}$`来区分
    * 从上到下是各个特征的输入或输出，相关于特征个数n，他们用右上角`$^{[i]}$`来区分

#### 1.3.5 Justification for vectorized implementation
1. 前面我们已经在每一节课都认真地分析了总的训练集数据如何组织与为何这样组织，所以这节课可以说是非常轻松了
2. 我们将`$w^T$`按行组织成一个矩阵，`$x$`等训练数据按列组织成一个矩阵，至于为什么，我们应该已经很清楚了，`$W$`中的一行对应于一个输出结点，与`$X$`中的一列相乘自然就得到了一个输出结点的一次输出数据，我们将此行和此列横向平移和纵向平移，那么我们得到的数据自然就组成了一个矩阵，这个矩阵各个位置分别是啥，前面已经说过
3. 最后我们再通过Python的广播机制加一个b，就完成了

#### 1.3.6 Activation functions
1. 我们之前只用过sigmoid函数，但是事实上sigmoid函数很少使用的，相对来说tanh函数几乎总是比它表现的好，虽然tanh只是sigmoid向下平移变换就可以得来，但是tanh平均值是0而不是0.5，这使得数据可以直接中心化
2. 但是有一个例外情况，就是二分类，它最终需要一个0~1的值，而sigmoid函数本身就是0~1的值，所以我们可以用它作为输出层激活函数，而隐藏层往往使用别的激活函数
3. 另外我们之前有介绍过ReLu函数，这是一个很好的激活函数，我们更多情况都是使用这个激活函数
4. 除ReLu函数还有一个Leaky ReLu函数，不过它并不是特别的常用

#### 1.3.7 why need a nonlinear activation function?
为什么需要这么一个非线性的激活函数呢？课上已经说得很明白了，如果使用线性激活函数那么多层神经网络都可以看成一层神经网络了，所以我们不会在隐层上使用线性激活函数，不过输出层的话是可以考虑的

#### 1.3.8 Derivatives of activation functions
要用梯度下降法算反向传播的话，就一定要计算激活函数的导数咯
1. sigmoid：`$a(1-a)$`
2. tanh：`$1-a^2$`
3. ReLU：就是简单的分段函数，因为0处不可微，所以可以归到大于0或者小于0里面

#### 1.3.9 Gradient descent for neural networks
propagation：
1. `$Z^{[1]}=W^{[1]}x+b^{[1]}$`
2. `$A^{[1]}=\sigma(Z^{[1]})$`
3. `$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$`
4. `$A^{[2]}=\sigma(Z^{[2]})$`

back propagation：
1. `$dZ^{[2]}=A^{[2]}-Y$`
2. `$dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}$`
3. `$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)$`
4. `$dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})$` # 这里是逐元素相乘
5. `$dW^{[1]}=\frac{1}{m}dZ^{[1]}X^{T}$`
6. `$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)$`

这里先把我们需要的公式都列出来了，至于为什么，下节课再说（事实上我们每节课都有思索过这些问题，大致上已经知道了），有几点注意的如下：
1. axis=1表示水平相加求和，db自然是使用水平相加将所有样本数据加和
2. keepdims是防止Python输出那些古怪的秩数为1的矩阵
3. 为了防止Python输出古怪的矩阵，可以显示调用reshape

#### 1.3.10 Backpropagation intuition
没什么，有几个值得注意的地方
1. 反向传播公式2和5使用的是X的转置，那是因为我们当初一层时候使用的是`$dw=\frac{1}{m}Xdz^T$`现在的W是dw的转置组织起来的，自然要转置咯
2. 反向传播的时候我们可以通过增加对维度的判断（assert等），以消除bug
3. 输出层的梯度是用梯度下降法计算得来的，之后的每层都是根据W分配得来

#### 1.3.11 Random+Initialization
1. 如果使用全零初始化权重，那么每次反向传播的值将会均匀的分配到每个隐藏单元对应的权重上，再多的隐藏单元也只相当于一个隐藏单元
2. 我们可以使用随机初始化的权重以保证不会出现上面的情况，另外b是可以初始化为0的，因为梯度的分配与W有关，随机的W便可使b每次反向传播得到的不相同
3. 我们可以这么初始化权重：`w = np.random.rand(2, 2) * 0.01`，咦，为什么有这么个0.01，哦，原来是防止太大的初始值使得像tanh和sigmoid激活函数饱和，梯度太小

### 1.4 Deep Neural Networks
#### 1.4.1 Deep L-layer neural network
1. 我们用l表示层数索引
2. 其他的前面提到过，没啥可说的（我发现1.3.2那次整理真的很有必要啊，整理之后后面基本都是浏览而过了）

#### 1.4.2 Forward and backward propagation
我们很容易得到前向传播和反向传播的关系式，或者说，我们已经在1.3.9写的很明白了这里copy一下吧：  
propagation：
1. `$Z^{[l]}=W^{[l]}·A^{[l-1]}+b^{[1]}$`
2. `$A^{[1]}=g^{[l]}(Z^{[1]})$`

back propagation：
1. `$dZ^{[l]}=W^{[l]T}dZ^{[l]}*g^{[l]'}(Z^{[l]})$` # 这里是逐元素相乘
2. `$dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}$`
3. `$db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)$`
4. `$dA^{[l-1]}=W^{[l]T}dZ^{[l]}$`

* 前向传播的时候可以将Z和A缓存起来以便后向传播使用

上周作业已经将这个实现……（因为参考链接根本不告诉我作业题里哪些是要自己写的，哪些是直接给出的，我又最讨厌copy源代码，只好边看边加功能）

#### 1.4.3 Forward propagation in a Deep Network
emmmm 貌似没啥，到时候用for循环组织各层就好

#### 1.4.4 Getting your matrix dimensions right
维度：
1. `$W:(n^{[l]}, n^{[l-1]})$`
2. `$b:(n^{[l]}, 1)$` # Python广播可以自动扩展
3. `$Z A:(n^{[l]}, m)$`

#### 1.4.5 Why deep representations?
1. 深度神经网络是一个从具体到抽象的过程
2. 用人脸识别举例：比如说我们第零层是输入的特征，经过一层W的变换后自然得到了一些基本的线这样微小的，而且线性的特征，然后经过一层激活函数，继续传递到下一层，很明显，这一层可以得到上一层的不同组合了，可能我们得到了曲线，当层数不断提高，它所描述的特征越来越复杂，可能某一层已经足够描述某一器官比如鼻子的形状，某一层已经能表征出人脸的大概形状……
3. 用声音识别举例：还是一样，最开始可能只能识别声调高低，之后某层可能识别音位，再之后某层可能已经可以识别单词，最后直到完整的句子
4. 另外至于为什么层数多之后可以很有效地解决某些问题，可以参考电路理论，如果实现同样功能的神经网络，隐藏单元的数量可能成指数倍增长，反过来说就是，很多情况下，深层神经网络会比浅层神经网络好的多

#### 1.4.6 Building blocks of deep neural networks
我们从`$y$`和`$\hat{y}$`得到cost等信息，然后利用cost function的导数计算反向传播梯度下降的的值，也就是`$dA^{[l]}$`，之后，根据前面1.4.2的公式就好，另外`$dA^{[0]}$`算不算都可以，是个没用的值

#### 1.4.7 Parameters vs Hyperparameters
1. 超参数就是那些控制了我们最终需要的W、b的参数，比如学习率、梯度下降法循环的数量、隐藏层的数目、隐藏层单元数目、激活函数的选择，之后我们还会遇到更多
2. 如何选择呢？我们要不断尝试与观察，看最后得到的J是否有下降，如此迭代以找到最合适的超参数
3. 另外一个很重要的就是直觉，这当然是要反复尝试锻炼的啦

#### 1.4.8 What does this have to do with the brain?
关系？也许当初是模拟神经元所做的吧，但是神经元绝对比这复杂多了，也许这样真的可以模拟出来大脑，也许根本不一样。总之，深度学习可以达到大脑的效果绝对是夸大的说法，至少，现在是这样的

## 2 Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization
### 2.1 Practical aspects of Deep Learning
#### 2.1.1 Train / Dev / Test sets
1. 超参数的优化是一个反复迭代的过程，即便一个领域的人才，对这个领域有着丰富的经验，到了另外一个领域仍然需要这个过程
2. 事实上，这个优化过程更取决于数据集、计算机配置等因素
3. 我们大多数情况下将数据分为三部分
    * 训练集（training set） 用于训练
    * 验证集（dev set） 用于调优超参数，直至在该验证集验证下误差达到最小
    * 测试集（test set） 用于评估最终确定的模型的精确度（泛化能力）
4. 小数据量的情况下需要分配很大比例的数据用于测试，但是在当今的大数据时代，这样的做法属实是没太大的必要了，我们完全可以拿出更小的部分用以验证
5. 训练集和验证集要确保来自同一分布
6. 测试集并不是必要的，因为它只是对最终所选定的神经网络做出无偏评估

#### 2.1.2 Bias /Variance
1. 偏差？方差？有区别吗？如果就数学概念而言的话，感觉这两个提不到一块去，不过也确实，他们一个评估的是预测模型，一个评估的是样本数据，但是在ML这里我们会经常见到他们的
2. 让我们看下偏差方差与我们训练模型有着什么样的关系：  
![DeepLearning05](../Images/DeepLearning05.png)  
3. 我们再用举一个简单的例子，这次就和实际操作有点联系了：  

**Train set error**|1%|15%|15%|0.5%
---|---|---|---|---
**Dev ser error**|11%|16%|30%|1%
||高偏差|高偏差|高偏差+高方差|低偏差+低方差

4. 不过偏差与方差也是相对的，前面的假设是基于我们判断的误差接近于0%的前提下而言的，这里称为**最优误差**或者**贝叶斯误差**。但如果，贝叶斯误差比较高，比如说15%，此时再看上面的第二个模型，可以说方差与偏差都不高，算是比较合理的了

#### 2.1.3 Basic Recipe for Machine Learning
那么有误差和方差的时候怎么调整呢？
1. 先看**偏差**高不？高的话试试选择**新的网络**，比如增加更多**隐藏层或者隐藏单元**的网络，或者花费**更多时间**来训练网络
2. 当偏差降低到可以接受的数值时，我们利用验证集检查一下**方差**是否有问题，如果方差较高，最好的解决方案就是采用**更多数据**，当然这有点难度，因此我们也可以通过尝试**正则化**来减少过拟合，如果可以找到合适的神经网络框架，有时是可以一箭双雕，同时减少方差和误差的，不过这个过程没有什么捷径了，就是反复的重复尝试……

#### 2.1.4 Regularization
好难QAQ
1. 想解决**过拟合**问题嘛，大概就需要准备**更多数据**或者**正则化**了，但是我们往往没有获取更多的数据的条件，所以我们可以尝试使用正则化避免过拟合或减少网络误差
2. 我们这里在成本函数J上增加一个正则化参数`$\lambda$`，因为b的维度较低，我们完全可以不对它进行正则化
3. 我们通常使用L2正则化，它是`$\frac{\lambda}{2m}$`乘上`$w$`范数的平方，也就是每个元素平方之和，可以表示为`$w^Tw$`
4. 而L1正则化则是使用`$\frac{\lambda}{m}$`乘上`$\sum_{j=1}^{n_x}|w|$`，这样得到的`$w$`将会是系数的，也就是其中会有很多0，也许这会节省些内存吧，但是这并不是我们所关注的，所以我们没必要因此去使用它
5. `$lambda$`是Python中的一个保留字，我么可以使用`$lambd$`来代替它
6. 说这么多，怎么实现呢？我们先写下现在的成本函数：`$J(w^{[1]},b^{[1]},...,w^{[2]},b^{[2]})=\frac{1}{m}\sum_{i=1}^{n}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}\sum_{l=1}^L||W^{[l]}||_F^2$`，该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标F标注，嗯，先这么叫好了，那么如何实现梯度下降？？？嗯？这个问题应该问吗？求导呗！
7. `$dw^{[l]}=backprop+\frac{\lambda}{m}w^{[l]}$`，哦，这里的`$backprop$`是原来没有正则化的`$dw^{[l]}$`，然后我们就可以进行梯度递降了
8. `$w^{[l]} = w^{[l]} - \alpha dw^{[l]} = w^{[l]}-\frac{\alpha\lambda}{m}w^{[l]}-\alpha backprop = (1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha backprop$`，这样我们就在`$w^{[l]}$`前加了一个小于1的系数，也就是趋向于使`$w^{[l]}$`更小，因此，L2正则化也被称为“权重递减”，但是权重减不减什么的并不是我们所关注的，所以我们不必记住这个名字

#### 2.1.5 Why regularization reduces overfitting?
1. 我们使用L2正则化的时候会有参数`$\lambda$`，很明显这个参数越大的话，参数W会越来越接近于0，不过这有什么影响呢？我们直观的感受下
2. 这里以激活函数为tanh的情况进行举例：  
![DeepLearning06](../Images/DeepLearning06.png)  
我们知道`$z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$`，那么也就是说`$W^{[l]}$`很小的时候，`$z^{[l]}$`也会很小，那么再计算`$a^{[l]}$`时，我们要通过这么一个激活函数对吧，我们可以看到，在激活函数输入很小的时候，它是线性的，而线性的意味着什么呢？再多的隐藏层都可以看做一层，也就是，简化了模型，这就使得模型很可能处于高偏差的状态，然后我们通过减小`$\lambda$`，就可以调整到那么一个，合适的、Just right状态
3. 应用梯度下降的时候一定要记得加上正则化项，这样才能保证梯度下降，否则我们可能只能使得原来的梯度下降，并没有使新的梯度函数下降

#### 2.1.6 Dropout Regularization
下面介绍下一种正则化——Dropout（随机失活），这个理解起来相对于L2正则化简单得多
1. 我们每次随机地让一部分结点失去作用，比如说以0.5的概率，这样会使得我们的神经网络变的更加的简单、规模更小，然后我们用backprop方法进行训练
2. 那么如何实现呢？我们最常用的方法是inverted dropout（反向随机失活），我们使用一个三层（l=3）的网络来举例，我们定义下第三层的dropout向量`$d^{[3]}$`：  
`d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob`  
然后我们看下它时候小于某数，我们称之为keep-prob，比如0.5？0.9？它表示保留某个隐藏单元的概率，哦Python会把这个变成一个只有True和False的矩阵，然后怎么用呢？
3. 下面，我们只需要对a3进行一点小小的改变  
```Python
a3 = np.multiply(a3, d3) # a3 *= d3`
a3 /= keep_prob
```
完了
4. 上面第一步是让结点随机失活，第二步是让期望值保持原来的水平（否则会损失一部分的）
5. 不过我们测试的时候可不能加上d的影响， 不然结果会是一个，随机的结果

#### 2.1.7 Understanding Dropout
1. dropout使得神经网络不依赖于某个输入单元，不会给他过多的权重，因为它随时有可能被清除
2. dropout可以为每一层设置不同的keep-prob，我们经常为隐藏单元比较多的层设置较小的keep-prob，以缓和该层的过拟合现象，而过拟合现象不严重的层可以设置为较高的数值，比如0.9、1.0，1.0的话就表示该层将全部保留
3. 不过这也带来一个问题就是，我们要调的超参数更多了
4. 我们通常在计算机视觉上使用dropout
5. dropout一大缺点是成本函数J不再被明确定义，因为每次都是随机的，我们很难明确的定义J使之每次迭代都下降，所以我们可以关闭dropout，也就是keep-prob设为1，确保J单调递减再打开dropout

#### 2.1.8 Other regularization methods
1. 数据扩增  
再拿猫的识别举例子，当我们数据不够时，我们可以将猫的图片水平翻转或者转动一定角度、放缩等操作，当然我们要保证他还是一只猫，比如竖直翻转什么的就是有点蠢的操作了
2. early stopping  
就是在过拟合之前就、停止继续训练了，但是这个缺点也很明显，就是很可能停下来的位置的J还是很高，但为了防止过拟合不得不过早停下来

#### 2.1.9 Normalizing inputs
归一化输入，它包括这么两步：
1. 零均值  
`$\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}$`  
`$x-=\mu$`
2. 归一化方差  
`$\sigma^2=\frac{1}{m}\sum_{i=1}^mx^{(i)}**2$`  
`$x/=\sigma^2$`  
![DeepLearning05](../Images/DeepLearning05.png)  

* 这样我们就得到了一个，均值为0，方差为1的输入特征，当然，我们要对每个输入特征都进行归一化，这样才能让每一个输入都很……嗯……规范……那么有什么好处呢？？？

* 我们首先看上面第一个“碗”，很明显，这是一个长相很奇怪的代价函数，而且它不是归一化的，如果我们使用梯度下降，那么他将不断地沿着梯度最大（这里体现为“等高线”的法线方向）崎岖的、蜿蜒的路走下去，而且这样我们没办法使用过大的步长（学习率），否则很难到达“终点”
* 我们再看第二个圆整的“碗”，我们直观上会很喜欢这样的一个碗，因为它很标准，每个方向上都是这样，任何一个初始值都会直接奔向“终点”，这使得J的优化会加快很多

* 另外，我们是在各特征的范围差距比较大时使用这个是非常必要的，如果各输入特征范围相似度比较高，那么我们使用归一化就不是那么重要了

#### 2.1.10 Vanishing / Exploding gradients
为什么会产生梯度消失和梯度爆炸现象呢？看下面的例子
1. 我们有那么一个**层数很多**的，线性激活的矩阵，我们假设每层的参数w都是稍稍小于1的对角矩阵，b当0处理，嗯，w只需要稍稍小于1，当传递到最后一层会是什么情况呢？不用说也能想象得到，那么我们又如何得到梯度优化参数呢？
2. 就是这样的过程，相反地，如果参数w稍稍大于1，那么最后一层的数据将会是爆炸式的

#### 2.1.11 Weight Initialization for Deep NetworksVanishing / Exploding gradients
1. 如果是一个神经元，我们可能不会希望随着输入层数量的增加而产生过高的输出，所以我们也许会希望除以输入层特征的数量
2. 让我们来计算一下，以免看不懂后面的根号是怎么来的  
    1. 然后就算自闭了……
    2. z = wx+~~b~~ = `$w_1x_1+w_2x_2+...+w_nx_n$`
    3. `$a = \sigma(z)$`
    4. `$D(z)$`  
    `$=D(\sum_{i=1}^nw_ix_i)$`  
    `$=\sum_{i=1}^nD(w_ix_i)$` # 相互独立  
    `$=nD(w_ix_i)$` # 同分布
    5. `$D(w_ix_i)$`  
    `$=E(w_i^2x_i^2)-E^2(w_i)E^2(x_i)$`  
    `$=E(w_i^2)E(x_i^2)-E^2(w_i)E^2(x_i)$`  
        > 假设：  
        `$x$`已经均一化 => `$E(x_i)=0,D(x_i)=1$`  
        `$w$`是以0为均值进行的初始化 => `$E(w_i)=0$`  

        `$=D(w_i)$`
    6. 即`$D(z)=nD(w_i)$`
        > 我们更期望每层得到的方差不会累积增加，维持在“1”这个Just right刚刚好（与`$D(x_i)$`一样）
        
        `$D(w_i)=\frac{1}{n}$` # 根据`$D(aX)=a^2D(X)$`很容易得出，`$w_i$`应每个除`$\sqrt{n}$`
    7. 完了？？？啊不对，我在算什么啊，下一层可不是z哦，下一层是a啊，那好说，其实上面的过程是使用线性激活的情况，非线性激活也是一样的步骤啦，下面试着推下ReLU作为激活函数的情况
    8. `$D(a)$`  
    `$=D\{\sum_{i=1}^nReLU(w_ix_i)\}$`  
    `$=nD\{ReLU(w_ix_i)\}$`
    9. `$D\{ReLU(w_ix_i)\}$`  
    `$=E\{ReLU^2(w_ix_i)\}-E^2\{ReLU(w_ix_i)\}$`  
    `$=\frac{1}{2}E(w_i^2x_i^2)+\frac{1}{2}E(0)-[\frac{1}{2}E(w_ix_i)+\frac{1}{2}E(0)]^2$`  
    `$=\frac{1}{2}E(w_i^2)E(x_i^2)-\frac{1}{4}E^2(w_ix_i)$`  
    `$=\frac{1}{2}D(w_i)$`
    10. `$D(a)=\frac{n}{2}D(w_i)$`  
        `$D(w_i)=\frac{2}{n}$`
    11. 就是这样，我们就算出来了ReLU的随机初始化方差应该取`$\frac{2}{n}$`，也就是`$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}})$`
    12. 当然，这是ReLU的情况，它是使用`$\sqrt{\frac{2}{n^{[l-1]}}}$`，还有其他的激活函数呢，对于tanh，有人说使用`$\sqrt{\frac{1}{n^{[l-1]}}}$`，也有人提到使用`$\sqrt{\frac{2}{n^{[l-1]}+n^{[l]}}}$`
    13. 不过，权重的初始化只是一个起点，这些公式只能给一个好的起点，他们会给出初始化权重矩阵的方差的默认值，如果想增加方差，那么方差会成为另一个超参数，这可以通过在刚才式子上增加一个调优参数实现，但这并不是我们的首要调优参数，所以，我们往往将它的优先级放的比较低。
    12. 边复习边算的，很多东西都忘了，可信度不高(～￣▽￣)～ 

#### 2.1.12 Numerical approximation of gradients
1. 这节课是讲解梯度检验的原理，其实就是导数的定义啦
2. I'm closing...  
![DeepLearning08](../Images/DeepLearning08.jpg)  
简单地算了下（实际上是画了下）为啥双边误差比单边误差小，至于为啥双边误差是`$O(\varepsilon^2)$`，单边误差是`$O(\varepsilon)$`，就想不出来了，暂时先这样，等后期开始复习高数再想想

#### 2.1.13 Gradient checking
1. 就是利用刚刚所说的双边误差对backprop的梯度进行检验  
![DeepLearning09](../Images/DeepLearning09.png)  
2. 按某种规则用`$\theta$`将所有参数（`$w_1\ b_1\ w_2\ b_2\ ...\ w_n\ b_n$`）组织起来，嗯，是一个很大的向量，按同样规则用`$d\theta$`将所有反向传播参数（`$dw_1\ db_1\ dw_2\ db_2\ ...\ dw_n\ db_n$`）
3. `$d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,...\theta_i+\varepsilon...)-J(\theta_1,\theta_2,...\theta_i-\varepsilon...)}{2\varepsilon}$`，这里的`$\varepsilon$`只在`$\theta_i$`上加，也就是只对`$\theta_i$`求的偏导（`$\frac{\partial J}{\partial \theta_i}$`，用双边的导数定义近似求解），相应地，因为`$d\theta$`也是这样维度的向量，`$d\theta[i]$`就表示的是我们自己根据反向传播计算的对`$\theta_i$`的求导（`$dw_1\ db_1\ dw_2\ db_2\ ...\ dw_n\ db_n$`都是一步步按反向传播计算出来的，虽然现在按照某种变换改变了，但他们都是对J的导数是没有问题的）
4. 我们要求`$d\theta_{approx}$`和`$d\theta$`，但我们现在只有各个偏导组成的向量，要怎么度量两个向量是否彼此接近呢？我们可以使用`$d\theta_{approx}[i]-d\theta[i]$`的欧几里得范数`$||d\theta_{approx}-d\theta||_2$`，然后比向量的长度进行归一化，也就是`$\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||2+||d\theta||_2}$`，Python的话，可以使用`np.linalg.norm(grad)`
来求范数
5. 我们看下这个值是否有问题，可以试着将它与`$10^{-7}$`比较一下，一般比它小的话就说明导数很可能是正确的，但如果高于它，很可能是出bug了，去调试吧！

#### 2.1.14 Gradient Checking Implementation Notes
1. 因为梯度检验真的只是对梯度进行检验，没必要在训练的时候也进行，它只是帮我们调试与排除一个bug的方法，对训练本身并没有用处，如果真的在训练的时候使用了的话，很可能会花费大量的时间在上面
2. 梯度检验也可以帮助我们大概地定位bug的位置，因为如果某个地方出bug了，那附近的`$d\theta_{approx}$`和`$d\theta$`相差的会比较大
3. 如果使用正则化，那么在梯度检验的时候不要忘了正则项
4. 如果使用dropout，需要先把dropout关闭，然后再进行梯度检验
5. 有的时候会出现这样的情况：`$w$`和`$b$`比较小时，梯度下降是正确的，运行梯度下降的时候，`$w$`和`$b$`会变得更大，会越来越不准确，为了防止出现这种情况，需要在**随机初始化过程中运行梯度检验，训练网络一段时间后再运行梯度检验**

### 2.2 Optimization algorithms
#### 2.2.1 Mini-batch gradient descent
Mini-batch？好奇怪的名字，但如果我们把以前的方法称作batch，Mini-batch就很好理解了。
1. batch每次是使用整个样本集m条数据对网络进行训练，但当m比较大的时候呢？这时每次训练完整个样本才能进行一次的梯度下降法，效率就会很低
2. 当样本集数量比较大的时候，我们将样本集拆分为一个个的**mini-batch**，我们每对一个mini-batch使用梯度递法，这样会很有效地加快训练的速度
3. 比如说有5,000,000个样本数据，我们可以将`$x^{(1)}$`到`$x^{(1000)}$`作为第一个mini-batch，我们把它记作`$X^{\{1\}}$`，同样地，`$x^{(1001)}$`到`$x^{(2000)}$`称为`$X^{\{2\}}$`and so on.直到`$X^{\{5000\}}$`，相应的，对y也做同样的处理；当然，我们现在需要用一个for循环来遍历整个训练集了
4. 我们现在需要如何计算成本？因为自己规模是1000，`$J=\frac{1}{1000}\sum_{i=1}^lL(\hat{y}^{(i)},y^{(i)})$`，当然，`$L(\hat{y}^{(i)},y^{(i)})$`是来自mini-batch`$X^{\{t\}}$`和`$Y^{\{t\}}$`中的样本
5. 如果用到了正则化，可以使用`$J=\frac{1}{1000}\sum_{i=1}^lL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2\cdot1000}\sum_l||\omega^{[l]}||_F^2$`

#### 2.2.2 Understanding mini-batch gradient descent
batch梯度下降时每次迭代成本都会下降（除非学习率过大导致在最优解处不收敛），但如果是mini-batch，我们更可能看到的是一个整体趋势减小但噪声很大的成本
1. 我们如果选取1作为mini-batch的话，噪声会非常非常的大
2. 如果选取m作为mini-batch也就是m的话，也就是batch梯度下降法，噪声是小了，但是单次迭代耗时太长
3. 我们通常在样本**小于2000的时候直接使用batch梯度下降法**，**如果大于2000的时候设mini-batch为64到512**，因为考虑到电脑内存的设置和使用方式，我们最好使用这些2的n次方型数据
4. 最后需要注意的一点是`$X^{\{t\}}$`和`$Y^{\{t\}}$`需要符合CPU/GPU内存

#### 2.2.3 Exponentially weighted averages
指数加权平均，也叫指数加权移动平均
1. 首先，我们有这么一堆数据，哦，看起来像一些散点图，那么如何求他们的指数加权平均呢
2. 
```math
v_0 = 0

v_1 = 0.9v_0+0.1\theta_1

v_1 = 0.9v_1+0.1\theta_2

v_1 = 0.9v_2+0.1\theta_3

\cdots

v_t = 0.9v_{(t-1)}+0.1\theta_t

```
* 这里`$\theta_t$`表示第t天的数据，`$v_t$`就是指数加权平均数
* 如果我们令这里的0.9为`$\beta$`，那么我们会得到
```math
v_0 = 0

v_t = \beta v_{(t-1)} + (1-\beta)\theta_t
```
* 可以看做最近`$\frac{1}{1-\beta}$`天的平均值

3. `$\beta$`越大每个数值平均的天数越多，曲线越光滑，`$\beta$`越小每个数值平均的天数越少，故曲线噪声很大

#### 2.2.4 Understanding exponentially weighted averages
1. 我们依然使用`$\beta=0.1$`作为例子
```math
v_{100} = 0.1\theta_{100} + 0.1\times0.9\theta_{99} + 0.1\times0.9^2\theta_{98} + 
0.1\times0.9^3\theta_{97} + 
0.1\times0.9^4\theta_{96} + 
\cdots
```
* 将`$v_{100}$`展开后很容易看出来这是`$\theta_t$`与一个指数函数相乘（`$\beta (1-\beta)^t$`，这里以100天为原点，向负方向增长）
* 那么为什么是表示最近`$\frac{1}{1-\beta}$`的平均值呢，我们推一下
    1. 首先，我们知道`$\beta$`是一个0-1的数，它更趋近于1（越趋近于0噪声越大，指数加权平均的意义越小）
    2. `$\lim_{n\rightarrow\infty} (1+\frac{1}{x})^x=e$`，emmm，很基本的极限公式
    3. `$\lim_{\beta\rightarrow1^-}\beta^{\frac{1}{1-\beta}}=\lim_{(\beta-1)\rightarrow0^-}[1+(\beta-1)]^{-\frac{1}{(\beta-1)}} = \frac{1}{e}$`
    4. 也就是说`$\frac{1}{1-\beta}$`天后的权重降低到了`$(1-\beta)\cdot\frac{1}{e}$`之下，可以说是一个足够小的值了
2. 不过，我们如何计算它呢？
```
v = 0
for loop{
    v = beta * v + (1-beta) * theta_t
}
```
3. 由上面的实现方法我们可以知道这种算法很少占用内存，计算的时候只需要读入一个数据并使用计算后的结果覆盖掉原来的数据

#### 2.2.5 Bias correction in exponentially weighted averages
这里讲下对早期数据偏差大的一个优化方案
1. 很明显，令`$v_0=0$`的话，起点会有点偏低，这导致前期的数据整体处于一个偏低的趋势，当然，后期数据会逐渐弱化掉前期数据的影响，但如果要考虑前期数据的话，我们需要做一下调整
2. 其实也没啥，就是令我们原来求得的`$v_t$`变成`$\frac{v_t}{1-\beta^t}$`，那么`$v_1=\frac{\beta*v_0+(1-\beta)\theta_1}{1-\beta^1}=\theta_1$`，嗯，第一个值已经修正到`$\theta_1$`了，那么后期数据呢？当`$t\rightarrow+\infty$`时，修正分母趋近于1，和原曲线是重合的
3. 注意，是使原来的数据除以`$1-\beta^t$`进行修正，修正后的数据并不会参与之后的运算，伪码描述大概是这样：
```
v = 0
for loop{
    v = beta * v + (1-beta) * theta_t
}
v /= 1-beta**t // 注意，不是放在forloop内的哦
```

#### 2.2.6 Gradient descent with Momentum
动量梯度下降法？听着很好玩，让我们来看看到底是个什么东西
1. 我们使用梯度递降法的时候经常会遇到这种情况：
![DeepLearning10](../Images/DeepLearning10.png)  
    * 每次梯度递降都会不断地波动，指向的方向并不是最终的位置，当然，增大学习率会使得这个现象更加严重
    * 我们更希望图中竖直方向上的波动更小些，水平方向移动更快些，要如何做呢？
2. 我们试试这个方法：
```math
v_{dW} = \beta v_{dW} + (1-\beta)dW

v_{db} = \beta v_{db} + (1-\beta)db
```
当然，我们后来梯度递降时候也相应地变成了：
```math
W = W - \alpha v_{dW}

b = b - \alpha v_{db}
```
* 不过这有什么好处呢？很明显，在梯度递降的过程中，由于移动平均值的相互抵消，我们在纵轴方向上的移动变缓了，又由于`$\alpha$`是不变的，我们水平方向上必然会变快，所以最终得到的曲线会像红线这样：  
![DeepLearning11](../Images/DeepLearning11.png)  
3. 不过为啥叫动量梯度递降呢？我们可以这样理解：
    * 一个小球从山上向下滚
    * 以`$v_{dW} = \beta v_{dW} + (1-\beta)dW$`为例
    * `$v_{dW}$`看作是速度，`$dW$`看作是加速度，`$\beta$`看作一个阻力的系数吧，也可以理解成摩擦力什么的，不过这不太严谨
    * 很明显，移动平均后的值使得每次梯度递降的部分受到前几次的影响，类似于速度这样的“连续”的物理量，不像单纯的梯度递降法每次都是独立的
4. 怎么计算呢？我们现在在梯度递降的过程中有了`$\alpha$`和`$\beta$`这两个超参数，嗯，计算方法前面已经写出来了，这里再整理下：
```math
v_{dW} = \beta v_{dW} + (1-\beta)dW

v_{db} = \beta v_{db} + (1-\beta)db

W = W - \alpha v_{dW}, b = b - \alpha v_{db}
```
* 我们经常取`$\beta=0.9$`
* 至于偏差修正，我们很少使用它，因为那只影响前几次，如果取`$\beta=0.9$`的话，那只影响前十次的`$v_{dW}$`，所以并没有太大必要去使用它
* 有时我们会看到去掉`$(1-\beta)$`的公式，emmm，就是：
```math
v_{dW} = \beta v_{dW} + dW

v_{db} = \beta v_{db} + db

W = W - \alpha v_{dW}, b = b - \alpha v_{db}
```
当然，它的原理还是Momentum法，只不过把`$(1-\beta)$`隐藏到后面的`$\alpha$`里了（**后面的推导没太大意义，可以略过**），当然其实`$\beta$`也是变了的，我们先只看`$dW$`，把这个式子改写成：
```math
v_{dW} = \beta' v_{dW} + dW

W = W - \alpha' v_{dW} = W - \alpha'(\beta'v_{dW} + dW)
```
与原来式子相比，我们可以知道：
```math
\alpha'(\beta'v_{dW} + dW) = \alpha(\beta v_{dW} + (1-\beta)dW)
```
易知：
```math
\alpha = \alpha'\beta'+\alpha'

\beta = \frac{\alpha'\beta'}{\alpha} = \frac{\alpha'\beta'}{\alpha'\beta'+\alpha'}
```
很明显，原来的`$\alpha$`与`$\beta$`是可以用现在的`$\alpha$`与`$\beta$`表示出来的，当然，这和原来的式子可以说没有任何区别，但是这个式子表意并不如原来的明确
5. 当然，我们在batch或者mini-batch都可以使用Momentum的方法

#### 2.2.7 RMSprop
和Momentum的图一样，我们参考那张图下
1. 我们假设横轴代表`$W$`，纵轴代表`$b$`，那么我们使用下面的方法试试：
```math
S_{dW} = \beta S_{dW} + (1-\beta)dW^2

S_{db} = \beta S_{db} + (1-\beta)db^2

W = W - \alpha \frac{dW}{\sqrt{S_{dW}}}, b = b - \alpha \frac{db}{\sqrt{S_{db}}}
```
2. 我们直观上这么理解这个方法：
    * 如果纵轴`$b$`上移动过快，那么`$S_{db}$`会是一个比较大的值，`$b$`梯度下降的梯度下降就会降低，`$W$`也是一样的，这样做就使得某一项移动不会过快，以消除摆动
    * 当然，这里只是为了简单只考虑了`$W$`和`$b$`，事实上`$W$`也是一个高维度的量，要考虑`$W_1, W_2, W_3, \cdots$`
    * 为了防止分母为0，我们经常使用在分母上加一个比较小的值`$\varepsilon$`（通常取`$10^{-8}$`）

#### 2.2.8 Adam optimization algorithm
Adam优化算法，简单的说，这是一个结合了Momentum算法和RMSprop算法的算法，直接上算法，相信大家也明白
```math
Initialize:v_{dW} = 0, S_{dW} = 0, v_{db} = 0, S_{db} = 0

v_{dW} = \beta_1v_{dW} + (1-\beta_1)dW, v_{db} = \beta_1v_{db} + (1-\beta_1)db

S_{dW} = \beta_2v_{dW} + (1-\beta_2)dW^2, S_{db} = \beta_2v_{db} + (1-\beta_2)db^2

v_{dW}^{corrected} = \frac{v_{dW}}{(1-\beta_1^t)}, v_{db}^{corrected} = \frac{v_{db}}{(1-\beta_1^t)}

S_{dW}^{corrected} = \frac{S_{dW}}{(1-\beta_2^t)}, S_{db}^{corrected} = \frac{S_{db}}{(1-\beta_2^t)}

W = W - \alpha\frac{v_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon}, 
b = b - \alpha\frac{v_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}
```
* Adam通常是使用偏差修正的
* Momentum和RMSprop的参数`$\beta$`是不一样的，故使用`$\beta_1$`和`$\beta_2$`以区分
* 这是一种能够适用于不同神经网络的算法
* 这里这么多超参数，我们如何设置超参数呢？
    1. `$\alpha$`，这是要经常调整的
    2. `$\beta_1$`，Adam论文作者推荐0.9
    3. `$\beta_2$`，Adam论文作者推荐0.999
    4. `$\varepsilon$`，Adam论文作者推荐`$10^{-8}$`

#### 2.2.9 Learning rate decay
很明显，我们学习率过大的时候很难收敛在最优解处，这时候如果我们使用递减的学习率，在接近最优解附近学习率已经很小了，就很容易收敛

下面是几种常用的计算方法
1. `$\alpha = \frac{1}{1+decayrate*epoch-num}\cdot\alpha_0$`
2. `$\alpha = 0.95^{epoch-num}\cdot\alpha_0$`
3. `$\alpha = \frac{k}{\sqrt{epoch-num}}\cdot\alpha_0$`
4. 按阶梯状下降的学习率……
5. 手动衰减😂

#### 2.2.10 The problem of local optima
1. 在我们的想象中，成本函数J和参数W之间的关系也许会更像这种形式：  
![DeepLearning12](../Images/DeepLearning12.png)  
也就是，有着很多个局部最优的解，这些局部最优点是满足什么条件的呢？很明显，在各个方向的偏导都是0，而且，在该点的凹凸形式也是一样的才行，不过在三维空间上这种情况还算是比较常见的，如果是在高维情况下，这种要求可以说是很苛刻的，如果在某一个点满足了各个方向偏微分是0，那么我们更多遇到的是这种情况：  
![DeepLearning13](../Images/DeepLearning13.png)  
也就是一个鞍点，很难碰到一个局部最优，因为在高维情况下这太难得了（`$2^n$`）

2. 虽然不会长时间困在局部最优的位置，但是，我们很可能长时间处在平缓段，就像下面这样：  
![DeepLearning14](../Images/DeepLearning14.png)  

因为梯度真的很小，所以梯度递降法会使它小心翼翼地沿着“脊”处缓缓移动，直到鞍点处出现微小扰动后才向正确方向走去，这种情况下，Adam算法可以让它更快地度过平缓段

3. 对高维的实在是难以直观地去理解，不过我们对它们的理解正在不断发展……………………



## Status  
***Studying...***  
181014 #1.1 Finished  
181017 #1.2 Finished  
181020 #1.3 Finished  
181021 [Week_3](https://github.com/SigureMo/notev/tree/master/Codes/DeepLearning/Week_3) Finished  
181022 #1.4 Finished  
181028 #2.1 Finished  
181116 #2.2 Finished  

## Reference
1. [深度学习工程师 - deeplearning.ai - 网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)
2. [黄海广博士笔记](http://www.ai-start.com/)
3. 《Python 神经网络编程》 Tariq Rashid