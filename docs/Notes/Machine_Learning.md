---
title: Machine Learning
date: 2019-12-02
categories:
   - 经纶·注
tags:
   - CS
   - ML
---

::: warning

Machine Learning 笔记

:::

<!-- more -->

## 监督学习

## 1 线性回归

主要用于拟合数据对的映射关系，甚至可以增加特征的高次项以进行非线性拟合

### 正则化

为了防止过拟合，直观的理解就是，我们需要尽可能减少高次项的参数，如果将参数的范数也放到 Cost 里，那么我们就会使得模型不至于那么复杂，以减轻过拟合现象

## 2 逻辑回归

主要用于分类，使用 Logistic 函数（Sigmoid 函数）将原输出作为一个概率进行输出（概率上的条件概率可证明）

我们不妨认为大于 0.5 便预测为正样本，小于 0.5 自然就是负样本，那么等于 0.5 在图形上将表征为决策边界，当然，我们可以根据实际情况调整这个 “0.5”

## 3 神经网络

当我们的输入特征比较多的时候，如果使用线性回归的话，高次项的参数量将会非常之多（比如有两个特征，将有 $x_1^2$ $x_2^2$ 还有相关项 $x_1 x_2$），而使用神经网络便可以解决该问题

神经网络由多层组成，前向传播是特征从左向右变换，反向传播是梯度从右向左传递

## 4 支持向量机（SVM）

支持向量机有着比逻辑回归更加简单的 Cost Function，这使得它会比逻辑回归计算量更小，另外，SVM 会有着比较大的间隔（margin），这使得 SVM 会选择比较更好的决策边界

为了学习到更加丰富的特征，需要使用核函数对原始特征进行加工，比如我们原始特征为 $x$ ，我们在向量空间中挑选几个标记 $l_1$ $l_2$ $l_3$，我们可以很容易的得到 $x$ 与这三个标记之间的距离，分别为 $f_1$ $f_2$ $f_3$，将 $[f_1, f_2, f_3]^T$ 记为 $f$，便得到了一个新的特征

那么如何挑选这些标记呢？我们可以从训练样本中挑选数据作为标记

一般情况下，SVM 比神经网络运算更快些，在一些比较简单的数据上，SVM 是更好的选择

## 无监督学习

## 5 聚类 —— K-means 聚类

所谓 K-means 即有 K 个聚类中心

### 如何运作？

Loop：

-  将样本分配给各个中心（按距离）
-  将各个中心移到该簇均值

### 如何初始化？

随机初始化

为避免全局最优，可多次随即初始化后，根据 J 来找最好的模型（只适用于 k 较小时）

### 如何选取 k

-  Plot 各个 k 的曲线，如果满足肘形图，可选拐点
-  根据后续需求选取

## 6 降维 —— 主成分分析方法（PCA）

-  数据压缩
-  可视化数据

### 如何运作？

试着找 k 个向量，data 到这些向量只有较小的距离

::: tip PCA 与线性回归的区别

-  线性回归的“距离”是各个样本到拟合数据的**竖直**距离
-  PCA 的“距离”是各个样本到向量的**垂直**距离

:::

### 如何选取 k

投影后误差的平方的平均值 / 数据方差 $\leq$ 0.01

为了保证数据压缩足够，k 尽量小嘛

### 数据压缩后如何恢复

逆运算，但会损失掉原数据与向量之间的误差，只是近似表示

### 如何应用？

> 只有在原始特征直接训练效果不好 / 太慢时才用

原始 $training\ set$ 的 $(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})$

对所有 $x$ （dim=10000）降维处理，得到 $z$ （dim=1000），然后对新的训练集 $(z^{(1)}, y^{(1)}), \cdots, (z^{(m)}, y^{(m)})$ 进行拟合

值得注意的是，PCA 中的参数是从 $training\ set$ 得到的，在 $dev\ set$ 和 $test\ set$ 仍然使用这个参数

# Reference

1. [机器学习 - Andrew Ng - 网易云课堂](https://study.163.com/course/courseLearn.htm?courseId=1004570029)
