{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Kannada MNIST](https://www.kaggle.com/c/Kannada-MNIST/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IW = 28\n",
    "IH = 28\n",
    "TP = 10\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        \n",
    "        root = 'data/Kannada-MNIST'\n",
    "        dig_file = os.path.join(root, 'Dig-MNIST.csv')\n",
    "        sample_file = os.path.join(root, 'sample_submission.csv')\n",
    "        train_file = os.path.join(root, 'train.csv')\n",
    "        test_file = os.path.join(root, 'test.csv')\n",
    "\n",
    "        for file in [dig_file, sample_file, train_file, test_file]:\n",
    "            assert os.path.exists(file), 'Please download dataset and save to \"data/Kannada-MNIST/\" before boot'\n",
    "\n",
    "        self.X_train, self.Y_train = self.read_csv(train_file, type='train')\n",
    "        self.X_test, _ = self.read_csv(test_file, type='test')\n",
    "        self.num_train_data, self.num_test_data = self.X_train.shape[0], self.X_test.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv(file_path, type='train'):\n",
    "        X = []\n",
    "        Y = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                print(f'get data {i}', end='\\r')\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                items = line.split(',')\n",
    "                if type == 'train':\n",
    "                    Y.append(np.array(items[0], dtype=np.int32))\n",
    "                X.append(np.array(items[1: ], dtype=np.float32).reshape((IH, IW, 1)) / 255.0)\n",
    "\n",
    "        X = np.array(X)\n",
    "        Y = np.eye(TP)[Y] if type == 'train' else None\n",
    "        return X, Y\n",
    "\n",
    "    def random_mini_batches(self, batch_size = 64):\n",
    "        \"\"\" 切分训练集为 mini_batch \"\"\"\n",
    "        \n",
    "        data_size = len(self.X_train)\n",
    "        permutation = list(np.random.permutation(data_size))\n",
    "        batch_permutation_indices = [permutation[i: i + batch_size] for i in range(0, data_size, batch_size)]\n",
    "        for batch_permutation in batch_permutation_indices:\n",
    "            yield self.X_train[batch_permutation], self.Y_train[batch_permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷积层神经元（卷积核）数目\n",
    "            kernel_size=[3, 3],     # 感受野大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "print_step = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epochs):\n",
    "    num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "    for j, (X, Y) in enumerate(data_loader.random_mini_batches(batch_size)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Y_ = model(X)\n",
    "            # loss = tf.reduce_mean(tf.square(Y - Y_))\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\n",
    "                y_true=Y,\n",
    "                y_pred=Y_\n",
    "            ))\n",
    "            if (i * data_loader.num_train_data + j) % print_step == 0:\n",
    "                print(f\"{i} - {j * batch_size: 5}: loss {loss.numpy()}\")\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}